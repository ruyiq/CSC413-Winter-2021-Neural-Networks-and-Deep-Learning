### Programming Assignment 3: Natural Language Processing and Multimodel Learning

#### Part 1 : Neural machine translation (NMT)
##### Q1 - Code a GRU Cell

<img src="mygrucell.png" alt="d1" width="600"/>

XX model performs significantly better because XXX.This might be because of XXX. 

##### Q2 - Identify failure modes
![alt text](grubestmodel.png "Title")
![alt text](grutranslate.png "Title")

##### Q3 - Comparing complexity

#### Part 2.1 : Additive Attention
##### Q3 

#### Part 2.2 : Scaled Dot Product Attention
##### Q1 - Implement the scaled dot-product attention mechanism
![alt text](ScaledDotAttention.png "Title")
##### Q2 - Implement the causal scaled dot-product attention mechanism
![alt text](CausalScaledDotAttention.png "Title")
##### Q3 
##### Q4
##### Q5
##### Q6
![alt text](save_loss_comparison_by_dataset.png "Title")
![alt text](save_loss_comparison_by_hidden.png "Title")

#### Part 3 : Fone-tuning Pretrained Language Models (LMs)
##### Q1 - Add a classifier to **BERT**
![alt text](BertForSentenceClassification.png "Title")
##### Q3 - Freezing the pretrained weights
##### Q4 - Effect of pretraining data.

#### Part 4 :  Connecting Text and Images with CLIP
##### Q2 - Prompting CLIP